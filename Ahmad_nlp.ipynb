{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOC67SRY8vf3DwpUsGnrSVg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lauradekker/Assembly_annotation/blob/master/Ahmad_nlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3EMWo2kUgWt6"
      },
      "outputs": [],
      "source": [
        "# NLP taks to train a model on relations in a sentence\n",
        "# 1. Upload JSON file into notebook and have data in different variables\n",
        "# 2. Output should be the relation between the words of the sentences\n",
        "# 3. Train model on train.json; and validate on dev.json\n",
        "# 4. Use ROBERTA model for the training -> sequence classification\n",
        "# 5. Use thigs such as HugginFace, transformers, torch\n",
        "# 6. Test the model on test.json using an F1 score\n",
        "# Deadline is June 18th"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "#open file\n",
        "json_file = open('train.json')\n",
        "#read data into variable\n",
        "json_df = pd.DataFrame(json.load(json_file))\n",
        "#load dictionary into a dataframe\n",
        "#json_df = pd.DataFrame(json_data)\n",
        "json_df = json_df.dropna()\n"
      ],
      "metadata": {
        "id": "kb0PbAZwhSik"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(json_df.shape) #58465 before NAs were dropped, same after\n",
        "\n",
        "tokens = list(json_df['token'])\n",
        "#print(tokens)\n",
        "#print(type(tokens))\n",
        "#tokens = list(tokens)\n",
        "print(tokens[0:10])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpGaTspiOrY1",
        "outputId": "de9f08ad-e8ed-4f9a-b94d-2b0cdea06479"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['Tom', 'Thabane', 'resigned', 'in', 'October', 'last', 'year', 'to', 'form', 'the', 'All', 'Basotho', 'Convention', '-LRB-', 'ABC', '-RRB-', ',', 'crossing', 'the', 'floor', 'with', '17', 'members', 'of', 'parliament', ',', 'causing', 'constitutional', 'monarch', 'King', 'Letsie', 'III', 'to', 'dissolve', 'parliament', 'and', 'call', 'the', 'snap', 'election', '.'], ['In', '1983', ',', 'a', 'year', 'after', 'the', 'rally', ',', 'Forsberg', 'received', 'the', 'so-called', '``', 'genius', 'award', \"''\", 'from', 'the', 'John', 'D.', 'and', 'Catherine', 'T.', 'MacArthur', 'Foundation', '.'], ['This', 'was', 'among', 'a', 'batch', 'of', 'paperback', 'Oxford', 'World', \"'s\", 'Classics', 'that', 'I', 'was', 'given', 'as', 'a', 'reward', 'for', 'reading', 'and', 'commenting', 'on', 'a', 'manuscript', 'for', 'OUP', '.'], ['The', 'latest', 'investigation', 'was', 'authorized', 'after', 'the', 'Supreme', 'Court', 'in', '2007', 'found', 'DCC', 'and', 'its', 'founder', ',', 'Jim', 'Flavin', ',', 'guilty', 'of', 'selling', 'DCC', \"'s\", '-LRB-', 'EURO', '-RRB-', '106', 'million', '-LRB-', 'then', '$', '130', 'million', '-RRB-', 'stake', 'in', 'Fyffes', 'after', 'Flavin', '--', 'also', 'a', 'Fyffes', 'director', 'at', 'the', 'time', '--', 'received', 'inside', 'information', 'about', 'bad', 'Fyffes', 'news', 'in', 'the', 'pipeline', '.'], ['The', 'event', 'is', 'a', 'response', 'to', 'a', 'White', 'House', 'immigration', 'reform', 'proposal', 'in', 'March', ',', 'said', 'Chung-Wha', 'Hong', ',', 'executive', 'director', 'of', 'the', 'New', 'York', 'Immigration', 'Coalition', '.'], ['Manning', 'was', 'prime', 'minister', 'in', '1991', ',', 'and', 'called', 'a', 'snap', 'elections', 'in', '1995', 'which', 'he', 'lost', 'to', 'the', 'UNC', 'after', 'the', 'party', 'entered', 'an', 'electoral', 'arrangement', 'with', 'the', 'National', 'Alliance', 'for', 'Reconstruction', '.'], ['Christine', 'Egerszegi-Obrist', '-LRB-', 'l', '-RRB-', 'and', 'Haddad-Adel'], ['Al-Hubayshi', 'explained', 'that', ',', 'far', 'from', 'being', 'a', 'mastermind', ',', 'Abu', 'Zubaydah', 'was', 'responsible', 'for', '``', 'receiving', 'people', 'and', 'financing', 'the', 'camp', ',', \"''\", 'that', 'he', 'once', 'bought', 'him', 'travel', 'tickets', ',', 'and', 'that', 'he', 'was', 'the', 'man', 'he', 'went', 'to', 'when', 'he', 'needed', 'a', 'replacement', 'passport', '.'], ['But', 'US', 'and', 'Indian', 'experts', 'say', 'it', 'has', 'hesitated', 'to', 'take', 'action', 'against', 'Lashkar-e-Taiba', ',', 'which', 'means', '``', 'The', 'Army', 'of', 'the', 'Pure', ',', \"''\", 'believing', 'that', 'the', 'Islamic', 'militants', 'could', 'prove', 'useful', 'in', 'pressuring', 'its', 'historic', 'rival', 'India', '.'], ['``', 'I', 'have', \"n't\", 'the', 'slightest', 'doubt', 'that', 'as', 'soon', 'as', 'US', 'and', 'Israeli', 'warships', 'are', 'in', 'place', '...', 'and', 'attempt', 'to', 'inspect', 'the', 'first', '-LRB-', 'Iranian', '-RRB-', 'merchant', 'ship', ',', 'a', 'shower', 'of', 'projectiles', 'will', 'be', 'unleashed', '...', 'That', 'will', 'be', 'the', 'exact', 'moment', 'the', 'terrible', 'war', 'will', 'start', ',', \"''\", 'Castro', 'wrote', 'in', 'the', 'Cubadebate', 'website', '.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "  print(json_data[i])\n",
        "\n",
        "print(len(json_data))\n",
        "print(type(json_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Zii7zipmKgd",
        "outputId": "35f69d29-5e43-4867-c521-06e7995ef52e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'id': '61b3a5c8c9a882dcfcd2', 'docid': 'AFP_ENG_20070218.0019.LDC2009T13', 'relation': 'org:founded_by', 'token': ['Tom', 'Thabane', 'resigned', 'in', 'October', 'last', 'year', 'to', 'form', 'the', 'All', 'Basotho', 'Convention', '-LRB-', 'ABC', '-RRB-', ',', 'crossing', 'the', 'floor', 'with', '17', 'members', 'of', 'parliament', ',', 'causing', 'constitutional', 'monarch', 'King', 'Letsie', 'III', 'to', 'dissolve', 'parliament', 'and', 'call', 'the', 'snap', 'election', '.'], 'subj_start': 10, 'subj_end': 12, 'obj_start': 0, 'obj_end': 1, 'subj_type': 'ORGANIZATION', 'obj_type': 'PERSON', 'stanford_pos': ['NNP', 'NNP', 'VBD', 'IN', 'NNP', 'JJ', 'NN', 'TO', 'VB', 'DT', 'DT', 'NNP', 'NNP', '-LRB-', 'NNP', '-RRB-', ',', 'VBG', 'DT', 'NN', 'IN', 'CD', 'NNS', 'IN', 'NN', ',', 'VBG', 'JJ', 'NN', 'NNP', 'NNP', 'NNP', 'TO', 'VB', 'NN', 'CC', 'VB', 'DT', 'NN', 'NN', '.'], 'stanford_ner': ['PERSON', 'PERSON', 'O', 'O', 'DATE', 'DATE', 'DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'ORGANIZATION', 'O', 'O', 'O', 'O', 'O', 'O', 'NUMBER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'PERSON', 'PERSON', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], 'stanford_head': [2, 3, 0, 5, 3, 7, 3, 9, 3, 13, 13, 13, 9, 15, 13, 15, 3, 3, 20, 18, 23, 23, 18, 25, 23, 3, 3, 32, 32, 32, 32, 27, 34, 27, 34, 34, 34, 40, 40, 37, 3], 'stanford_deprel': ['compound', 'nsubj', 'ROOT', 'case', 'nmod', 'amod', 'nmod:tmod', 'mark', 'xcomp', 'det', 'compound', 'compound', 'dobj', 'punct', 'appos', 'punct', 'punct', 'xcomp', 'det', 'dobj', 'case', 'nummod', 'nmod', 'case', 'nmod', 'punct', 'xcomp', 'amod', 'compound', 'compound', 'compound', 'dobj', 'mark', 'xcomp', 'dobj', 'cc', 'conj', 'det', 'compound', 'dobj', 'punct']}\n",
            "{'id': '61b3a65fb9b7111c4ca4', 'docid': 'NYT_ENG_20071026.0056.LDC2009T13', 'relation': 'no_relation', 'token': ['In', '1983', ',', 'a', 'year', 'after', 'the', 'rally', ',', 'Forsberg', 'received', 'the', 'so-called', '``', 'genius', 'award', \"''\", 'from', 'the', 'John', 'D.', 'and', 'Catherine', 'T.', 'MacArthur', 'Foundation', '.'], 'subj_start': 9, 'subj_end': 9, 'obj_start': 19, 'obj_end': 20, 'subj_type': 'PERSON', 'obj_type': 'PERSON', 'stanford_pos': ['IN', 'CD', ',', 'DT', 'NN', 'IN', 'DT', 'NN', ',', 'NNP', 'VBD', 'DT', 'JJ', '``', 'NN', 'NN', \"''\", 'IN', 'DT', 'NNP', 'NNP', 'CC', 'NNP', 'NNP', 'NNP', 'NNP', '.'], 'stanford_ner': ['O', 'DATE', 'O', 'DURATION', 'DURATION', 'O', 'O', 'O', 'O', 'PERSON', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'ORGANIZATION', 'ORGANIZATION', 'ORGANIZATION', 'ORGANIZATION', 'O'], 'stanford_head': [2, 11, 11, 5, 11, 8, 8, 5, 11, 11, 0, 16, 16, 16, 16, 11, 16, 21, 21, 21, 16, 21, 26, 26, 26, 21, 11], 'stanford_deprel': ['case', 'nmod', 'punct', 'det', 'nmod:tmod', 'case', 'det', 'nmod', 'punct', 'nsubj', 'ROOT', 'det', 'amod', 'punct', 'compound', 'dobj', 'punct', 'case', 'det', 'compound', 'nmod', 'cc', 'compound', 'compound', 'compound', 'conj', 'punct']}\n",
            "{'id': '61b3a65fb9aeb61c81e7', 'docid': 'eng-NG-31-126955-9171242', 'relation': 'no_relation', 'token': ['This', 'was', 'among', 'a', 'batch', 'of', 'paperback', 'Oxford', 'World', \"'s\", 'Classics', 'that', 'I', 'was', 'given', 'as', 'a', 'reward', 'for', 'reading', 'and', 'commenting', 'on', 'a', 'manuscript', 'for', 'OUP', '.'], 'subj_start': 26, 'subj_end': 26, 'obj_start': 7, 'obj_end': 8, 'subj_type': 'ORGANIZATION', 'obj_type': 'ORGANIZATION', 'stanford_pos': ['DT', 'VBD', 'IN', 'DT', 'NN', 'IN', 'NN', 'NNP', 'NNP', 'POS', 'NNS', 'IN', 'PRP', 'VBD', 'VBN', 'IN', 'DT', 'NN', 'IN', 'NN', 'CC', 'VBG', 'IN', 'DT', 'NN', 'IN', 'NN', '.'], 'stanford_ner': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'ORGANIZATION', 'ORGANIZATION', 'O', 'MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], 'stanford_head': [5, 5, 5, 5, 0, 11, 9, 9, 11, 9, 5, 15, 15, 15, 5, 18, 18, 15, 20, 18, 15, 15, 25, 25, 22, 27, 25, 5], 'stanford_deprel': ['nsubj', 'cop', 'case', 'det', 'ROOT', 'case', 'compound', 'compound', 'nmod:poss', 'case', 'nmod', 'mark', 'nsubjpass', 'auxpass', 'dep', 'case', 'det', 'nmod', 'case', 'nmod', 'cc', 'conj', 'case', 'det', 'nmod', 'case', 'nmod', 'punct']}\n",
            "{'id': '61b3a65fb9c9956eccbc', 'docid': 'APW_ENG_20100119.0780', 'relation': 'no_relation', 'token': ['The', 'latest', 'investigation', 'was', 'authorized', 'after', 'the', 'Supreme', 'Court', 'in', '2007', 'found', 'DCC', 'and', 'its', 'founder', ',', 'Jim', 'Flavin', ',', 'guilty', 'of', 'selling', 'DCC', \"'s\", '-LRB-', 'EURO', '-RRB-', '106', 'million', '-LRB-', 'then', '$', '130', 'million', '-RRB-', 'stake', 'in', 'Fyffes', 'after', 'Flavin', '--', 'also', 'a', 'Fyffes', 'director', 'at', 'the', 'time', '--', 'received', 'inside', 'information', 'about', 'bad', 'Fyffes', 'news', 'in', 'the', 'pipeline', '.'], 'subj_start': 55, 'subj_end': 55, 'obj_start': 28, 'obj_end': 29, 'subj_type': 'ORGANIZATION', 'obj_type': 'NUMBER', 'stanford_pos': ['DT', 'JJS', 'NN', 'VBD', 'VBN', 'IN', 'DT', 'NNP', 'NNP', 'IN', 'CD', 'VBD', 'NNP', 'CC', 'PRP$', 'NN', ',', 'NNP', 'NNP', ',', 'JJ', 'IN', 'VBG', 'NNP', 'POS', '-LRB-', 'NN', '-RRB-', 'CD', 'CD', '-LRB-', 'RB', '$', 'CD', 'CD', '-RRB-', 'NN', 'IN', 'NNP', 'IN', 'NNP', ':', 'RB', 'DT', 'NNP', 'NN', 'IN', 'DT', 'NN', ':', 'VBD', 'IN', 'NN', 'IN', 'JJ', 'NNP', 'NN', 'IN', 'DT', 'NN', '.'], 'stanford_ner': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'ORGANIZATION', 'ORGANIZATION', 'O', 'DATE', 'O', 'ORGANIZATION', 'O', 'O', 'O', 'O', 'PERSON', 'PERSON', 'O', 'O', 'O', 'O', 'ORGANIZATION', 'O', 'O', 'O', 'O', 'NUMBER', 'NUMBER', 'O', 'O', 'MONEY', 'MONEY', 'MONEY', 'O', 'O', 'O', 'ORGANIZATION', 'O', 'PERSON', 'O', 'O', 'O', 'ORGANIZATION', 'O', 'O', 'O', 'DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'ORGANIZATION', 'O', 'O', 'O', 'O', 'O'], 'stanford_head': [3, 3, 5, 5, 0, 9, 9, 9, 5, 11, 5, 5, 21, 13, 16, 13, 16, 19, 16, 16, 12, 23, 21, 23, 24, 27, 24, 27, 30, 27, 33, 33, 27, 35, 33, 33, 27, 39, 37, 41, 37, 27, 46, 46, 46, 24, 49, 49, 46, 5, 5, 53, 51, 57, 57, 57, 53, 60, 60, 57, 5], 'stanford_deprel': ['det', 'amod', 'nsubjpass', 'auxpass', 'ROOT', 'case', 'det', 'compound', 'nmod', 'case', 'nmod', 'dep', 'nsubj', 'cc', 'nmod:poss', 'conj', 'punct', 'compound', 'appos', 'punct', 'xcomp', 'mark', 'advcl', 'dobj', 'case', 'punct', 'dep', 'punct', 'compound', 'nummod', 'punct', 'advmod', 'dep', 'compound', 'nummod', 'punct', 'dep', 'case', 'nmod', 'case', 'nmod', 'punct', 'advmod', 'det', 'compound', 'acl:relcl', 'case', 'det', 'nmod', 'punct', 'dep', 'case', 'nmod', 'case', 'amod', 'compound', 'nmod', 'case', 'det', 'nmod', 'punct']}\n",
            "{'id': '61b3a65fb9197aba87ff', 'docid': 'APW_ENG_20070501.0905.LDC2009T13', 'relation': 'no_relation', 'token': ['The', 'event', 'is', 'a', 'response', 'to', 'a', 'White', 'House', 'immigration', 'reform', 'proposal', 'in', 'March', ',', 'said', 'Chung-Wha', 'Hong', ',', 'executive', 'director', 'of', 'the', 'New', 'York', 'Immigration', 'Coalition', '.'], 'subj_start': 23, 'subj_end': 26, 'obj_start': 13, 'obj_end': 13, 'subj_type': 'ORGANIZATION', 'obj_type': 'DATE', 'stanford_pos': ['DT', 'NN', 'VBZ', 'DT', 'NN', 'TO', 'DT', 'NNP', 'NNP', 'NN', 'NN', 'NN', 'IN', 'NNP', ',', 'VBD', 'NNP', 'NNP', ',', 'JJ', 'NN', 'IN', 'DT', 'NNP', 'NNP', 'NNP', 'NNP', '.'], 'stanford_ner': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'ORGANIZATION', 'ORGANIZATION', 'O', 'O', 'O', 'O', 'DATE', 'O', 'O', 'PERSON', 'PERSON', 'O', 'O', 'O', 'O', 'O', 'ORGANIZATION', 'ORGANIZATION', 'ORGANIZATION', 'ORGANIZATION', 'O'], 'stanford_head': [2, 5, 5, 5, 16, 12, 12, 12, 12, 12, 12, 5, 14, 12, 16, 0, 18, 16, 18, 21, 18, 27, 27, 27, 27, 27, 21, 16], 'stanford_deprel': ['det', 'nsubj', 'cop', 'det', 'ccomp', 'case', 'det', 'compound', 'compound', 'compound', 'compound', 'nmod', 'case', 'nmod', 'punct', 'ROOT', 'compound', 'nsubj', 'punct', 'amod', 'appos', 'case', 'det', 'compound', 'compound', 'compound', 'nmod', 'punct']}\n",
            "{'id': '61b3a65fb94b342e4da3', 'docid': 'AFP_ENG_20071105.0397.LDC2009T13', 'relation': 'no_relation', 'token': ['Manning', 'was', 'prime', 'minister', 'in', '1991', ',', 'and', 'called', 'a', 'snap', 'elections', 'in', '1995', 'which', 'he', 'lost', 'to', 'the', 'UNC', 'after', 'the', 'party', 'entered', 'an', 'electoral', 'arrangement', 'with', 'the', 'National', 'Alliance', 'for', 'Reconstruction', '.'], 'subj_start': 19, 'subj_end': 19, 'obj_start': 29, 'obj_end': 32, 'subj_type': 'ORGANIZATION', 'obj_type': 'ORGANIZATION', 'stanford_pos': ['VBG', 'VBD', 'JJ', 'NN', 'IN', 'CD', ',', 'CC', 'VBD', 'DT', 'NN', 'NNS', 'IN', 'CD', 'WDT', 'PRP', 'VBD', 'TO', 'DT', 'NNP', 'IN', 'DT', 'NN', 'VBD', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NNP', 'NNP', 'IN', 'NNP', '.'], 'stanford_ner': ['O', 'O', 'O', 'O', 'O', 'DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'DATE', 'O', 'O', 'O', 'O', 'O', 'ORGANIZATION', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'ORGANIZATION', 'ORGANIZATION', 'ORGANIZATION', 'ORGANIZATION', 'O'], 'stanford_head': [4, 4, 4, 0, 6, 4, 4, 4, 4, 12, 12, 9, 14, 9, 17, 17, 9, 20, 20, 17, 24, 23, 24, 17, 27, 27, 24, 31, 31, 31, 27, 33, 27, 4], 'stanford_deprel': ['nsubj', 'cop', 'amod', 'ROOT', 'case', 'nmod', 'punct', 'cc', 'conj', 'det', 'compound', 'xcomp', 'case', 'nmod', 'dobj', 'nsubj', 'ccomp', 'case', 'det', 'nmod', 'mark', 'det', 'nsubj', 'advcl', 'det', 'amod', 'dobj', 'case', 'det', 'compound', 'nmod', 'case', 'nmod', 'punct']}\n",
            "{'id': '61b3a65fb9062d316790', 'docid': 'eng-NG-31-142407-10043754', 'relation': 'no_relation', 'token': ['Christine', 'Egerszegi-Obrist', '-LRB-', 'l', '-RRB-', 'and', 'Haddad-Adel'], 'subj_start': 6, 'subj_end': 6, 'obj_start': 0, 'obj_end': 1, 'subj_type': 'PERSON', 'obj_type': 'PERSON', 'stanford_pos': ['NNP', 'NNP', '-LRB-', 'NN', '-RRB-', 'CC', 'NNP'], 'stanford_ner': ['PERSON', 'PERSON', 'O', 'O', 'O', 'O', 'O'], 'stanford_head': [2, 0, 4, 2, 4, 2, 2], 'stanford_deprel': ['compound', 'ROOT', 'punct', 'appos', 'punct', 'cc', 'conj']}\n",
            "{'id': '61b3a65fb919cf88f14e', 'docid': 'eng-NG-31-142686-10071439', 'relation': 'per:identity', 'token': ['Al-Hubayshi', 'explained', 'that', ',', 'far', 'from', 'being', 'a', 'mastermind', ',', 'Abu', 'Zubaydah', 'was', 'responsible', 'for', '``', 'receiving', 'people', 'and', 'financing', 'the', 'camp', ',', \"''\", 'that', 'he', 'once', 'bought', 'him', 'travel', 'tickets', ',', 'and', 'that', 'he', 'was', 'the', 'man', 'he', 'went', 'to', 'when', 'he', 'needed', 'a', 'replacement', 'passport', '.'], 'subj_start': 42, 'subj_end': 42, 'obj_start': 25, 'obj_end': 25, 'subj_type': 'PERSON', 'obj_type': 'PERSON', 'stanford_pos': ['NNP', 'VBD', 'IN', ',', 'RB', 'IN', 'VBG', 'DT', 'NN', ',', 'NNP', 'NNP', 'VBD', 'JJ', 'IN', '``', 'VBG', 'NNS', 'CC', 'VBG', 'DT', 'NN', ',', \"''\", 'IN', 'PRP', 'RB', 'VBD', 'PRP', 'VB', 'NNS', ',', 'CC', 'IN', 'PRP', 'VBD', 'DT', 'NN', 'PRP', 'VBD', 'TO', 'WRB', 'PRP', 'VBD', 'DT', 'NN', 'NN', '.'], 'stanford_ner': ['PERSON', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'PERSON', 'PERSON', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], 'stanford_head': [2, 0, 14, 14, 9, 9, 9, 9, 14, 14, 12, 14, 14, 2, 17, 17, 14, 17, 17, 17, 22, 20, 2, 23, 28, 28, 28, 24, 28, 28, 30, 30, 30, 38, 38, 38, 38, 30, 40, 38, 44, 44, 44, 40, 47, 47, 44, 28], 'stanford_deprel': ['nsubj', 'ROOT', 'mark', 'punct', 'advmod', 'mark', 'cop', 'det', 'advcl', 'punct', 'compound', 'nsubj', 'cop', 'ccomp', 'mark', 'punct', 'advcl', 'dobj', 'cc', 'conj', 'det', 'dobj', 'punct', 'root', 'mark', 'nsubj', 'advmod', 'root', 'dobj', 'dep', 'dobj', 'punct', 'cc', 'mark', 'nsubj', 'cop', 'det', 'conj', 'nsubj', 'acl:relcl', 'mark', 'advmod', 'nsubj', 'advcl', 'det', 'compound', 'dobj', 'punct']}\n",
            "{'id': '61b3aa9e363a44f45c48', 'docid': 'AFP_ENG_20100414.0728', 'relation': 'org:alternate_names', 'token': ['But', 'US', 'and', 'Indian', 'experts', 'say', 'it', 'has', 'hesitated', 'to', 'take', 'action', 'against', 'Lashkar-e-Taiba', ',', 'which', 'means', '``', 'The', 'Army', 'of', 'the', 'Pure', ',', \"''\", 'believing', 'that', 'the', 'Islamic', 'militants', 'could', 'prove', 'useful', 'in', 'pressuring', 'its', 'historic', 'rival', 'India', '.'], 'subj_start': 13, 'subj_end': 13, 'obj_start': 19, 'obj_end': 22, 'subj_type': 'ORGANIZATION', 'obj_type': 'ORGANIZATION', 'stanford_pos': ['CC', 'NNP', 'CC', 'JJ', 'NNS', 'VBP', 'PRP', 'VBZ', 'VBN', 'TO', 'VB', 'NN', 'IN', 'NNP', ',', 'WDT', 'VBZ', '``', 'DT', 'NNP', 'IN', 'DT', 'NNP', ',', \"''\", 'VBG', 'IN', 'DT', 'JJ', 'NNS', 'MD', 'VB', 'JJ', 'IN', 'VBG', 'PRP$', 'JJ', 'JJ', 'NNP', '.'], 'stanford_ner': ['O', 'LOCATION', 'O', 'MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'LOCATION', 'O'], 'stanford_head': [6, 6, 2, 5, 2, 0, 9, 9, 6, 11, 9, 11, 14, 11, 14, 17, 14, 17, 20, 17, 23, 23, 20, 6, 6, 6, 32, 30, 30, 32, 32, 26, 32, 35, 32, 39, 39, 39, 35, 6], 'stanford_deprel': ['cc', 'nsubj', 'cc', 'amod', 'conj', 'ROOT', 'nsubj', 'aux', 'ccomp', 'mark', 'xcomp', 'dobj', 'case', 'nmod', 'punct', 'nsubj', 'acl:relcl', 'punct', 'det', 'dobj', 'case', 'det', 'nmod', 'punct', 'punct', 'xcomp', 'mark', 'det', 'amod', 'nsubj', 'aux', 'ccomp', 'xcomp', 'mark', 'advcl', 'nmod:poss', 'amod', 'amod', 'dobj', 'punct']}\n",
            "{'id': '61b3a65fb92c8c9d3d94', 'docid': 'AFP_ENG_20100628.0710', 'relation': 'no_relation', 'token': ['``', 'I', 'have', \"n't\", 'the', 'slightest', 'doubt', 'that', 'as', 'soon', 'as', 'US', 'and', 'Israeli', 'warships', 'are', 'in', 'place', '...', 'and', 'attempt', 'to', 'inspect', 'the', 'first', '-LRB-', 'Iranian', '-RRB-', 'merchant', 'ship', ',', 'a', 'shower', 'of', 'projectiles', 'will', 'be', 'unleashed', '...', 'That', 'will', 'be', 'the', 'exact', 'moment', 'the', 'terrible', 'war', 'will', 'start', ',', \"''\", 'Castro', 'wrote', 'in', 'the', 'Cubadebate', 'website', '.'], 'subj_start': 52, 'subj_end': 52, 'obj_start': 26, 'obj_end': 26, 'subj_type': 'PERSON', 'obj_type': 'NATIONALITY', 'stanford_pos': ['``', 'PRP', 'VBP', 'RB', 'DT', 'JJS', 'NN', 'IN', 'RB', 'RB', 'IN', 'NNP', 'CC', 'NNP', 'NNS', 'VBP', 'IN', 'NN', ':', 'CC', 'NN', 'TO', 'VB', 'DT', 'JJ', '-LRB-', 'JJ', '-RRB-', 'NN', 'NN', ',', 'DT', 'NN', 'IN', 'NNS', 'MD', 'VB', 'VBN', ':', 'DT', 'MD', 'VB', 'DT', 'JJ', 'NN', 'DT', 'JJ', 'NN', 'MD', 'VB', ',', \"''\", 'NNP', 'VBD', 'IN', 'DT', 'NNP', 'NN', '.'], 'stanford_ner': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'LOCATION', 'O', 'MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'ORDINAL', 'O', 'MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'PERSON', 'O', 'O', 'O', 'MISC', 'O', 'O'], 'stanford_head': [0, 7, 7, 7, 7, 7, 1, 54, 10, 45, 18, 15, 12, 12, 18, 18, 18, 10, 10, 10, 38, 23, 21, 30, 30, 30, 30, 30, 30, 23, 21, 33, 38, 35, 33, 38, 38, 10, 45, 45, 45, 45, 45, 45, 54, 48, 48, 50, 50, 45, 54, 54, 54, 7, 58, 58, 58, 54, 7], 'stanford_deprel': ['ROOT', 'nsubj', 'aux', 'neg', 'det', 'amod', 'root', 'mark', 'advmod', 'advmod', 'mark', 'compound', 'cc', 'conj', 'nsubj', 'cop', 'case', 'advcl', 'punct', 'cc', 'dep', 'mark', 'acl', 'det', 'amod', 'punct', 'amod', 'punct', 'compound', 'dobj', 'punct', 'det', 'nsubjpass', 'case', 'nmod', 'aux', 'auxpass', 'conj', 'punct', 'nsubj', 'aux', 'cop', 'det', 'amod', 'ccomp', 'det', 'amod', 'nsubj', 'aux', 'ccomp', 'punct', 'punct', 'nsubj', 'ccomp', 'case', 'det', 'compound', 'nmod', 'punct']}\n",
            "58465\n",
            "<class 'list'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RoBERTa Token Classification example\n",
        "\n",
        "from transformers import AutoTokenizer, RobertaForTokenClassification\n",
        "import torch\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Jean-Baptiste/roberta-large-ner-english\")\n",
        "model = RobertaForTokenClassification.from_pretrained(\"Jean-Baptiste/roberta-large-ner-english\")\n",
        "\n",
        "inputs = tokenizer(\"HuggingFace is a company based in Paris and New York\", add_special_tokens=False, return_tensors=\"pt\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    logits = model(**inputs).logits\n",
        "\n",
        "predicted_token_class_ids = logits.argmax(-1)\n",
        "\n",
        "predicted_tokens_classes = [model.config.id2label[t.item()] for t in predicted_token_class_ids[0]]\n",
        "print(predicted_tokens_classes)\n",
        "\n",
        "labels = predicted_token_class_ids\n",
        "loss = model(**inputs, labels=labels).loss\n",
        "round(loss.item(), 2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_b7RlTorH2mk",
        "outputId": "f426771c-7f47-4c5d-99f1-fb134ee7657c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['O', 'ORG', 'ORG', 'O', 'O', 'O', 'O', 'O', 'LOC', 'O', 'LOC', 'LOC']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.01"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RoBERTa TF Token Classification example\n",
        "\n",
        "from transformers import AutoTokenizer, TFRobertaForTokenClassification\n",
        "import tensorflow as tf\n",
        "\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"Jean-Baptiste/roberta-large-ner-english\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"ydshieh/roberta-large-ner-english\")\n",
        "#\n",
        "#model = RobertaForTokenClassification.from_pretrained(\"Jean-Baptiste/roberta-large-ner-english\")\n",
        "model = TFRobertaForTokenClassification.from_pretrained(\"ydshieh/roberta-large-ner-english\")\n",
        "#\n",
        "\n",
        "#input the token key here?\n",
        "inputs = tokenizer(\"HuggingFace is a company based in Paris and New York\", add_special_tokens=False, return_tensors=\"tf\")\n",
        "\n",
        "#with torch.no_grad():\n",
        "logits = model(**inputs).logits\n",
        "\n",
        "#predicted_token_class_ids = logits.argmax(-1)\n",
        "predicted_token_class_ids = tf.math.argmax(logits, axis=-1)\n",
        "predicted_tokens_classes = [model.config.id2label[t] for t in predicted_token_class_ids[0].numpy().tolist()]\n",
        "predicted_tokens_classes\n",
        "#predicted_tokens_classes = [model.config.id2label[t.item()] for t in predicted_token_class_ids[0]]\n",
        "#predicted_tokens_classes\n",
        "\n",
        "labels = predicted_token_class_ids\n",
        "loss = tf.math.reduce_mean(model(**inputs, labels=labels).loss)\n",
        "round(float(loss), 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwpX6QaupOJ9",
        "outputId": "baf8bd73-b934-440c-831c-c034d8dc4287"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at ydshieh/roberta-large-ner-english were not used when initializing TFRobertaForTokenClassification: ['dropout_73']\n",
            "- This IS expected if you are initializing TFRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFRobertaForTokenClassification were initialized from the model checkpoint at ydshieh/roberta-large-ner-english.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForTokenClassification for predictions without further training.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.01"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RoBERTa Sequence Classification Example\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, RobertaForSequenceClassification\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-emotion\")\n",
        "model = RobertaForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-emotion\")\n",
        "\n",
        "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
        "#inputs\n",
        "\n",
        "with torch.no_grad():\n",
        "    logits = model(**inputs).logits\n",
        "\n",
        "predicted_class_id = logits.argmax().item()\n",
        "model.config.id2label[predicted_class_id] # returns 'optimism' in the example\n",
        "\n",
        "num_labels = len(model.config.id2label)\n",
        "model = RobertaForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-emotion\", num_labels=num_labels)\n",
        "\n",
        "labels = torch.tensor([1])\n",
        "loss = model(**inputs, labels=labels).loss\n",
        "round(loss.item(), 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txt_1H02ciVG",
        "outputId": "1b35cdb9-a4d6-4567-d2cf-e840bf9591d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.08"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RoBERTa Sequence Classification tokenize the dictionary\n",
        "\n",
        "import torch\n",
        "#from transformers import AutoTokenizer, RobertaForSequenceClassification\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "\n",
        "#additional input due to suggestions from stackoverflow to error\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-emotion\", add_prefix_space=True)\n",
        "#model = RobertaForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-emotion\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/roberta-base\", add_prefix_space=True)\n",
        "model = AutoModelForMaskedLM.from_pretrained(\"FacebookAI/roberta-base\")\n",
        "\n",
        "#additional input was put there at the suggestion of the error messages\n",
        "inputs = tokenizer(tokens, return_tensors=\"pt\", is_split_into_words=True, padding=True, truncation=True)\n",
        "\n",
        "with torch.no_grad():\n",
        "    logits = model(**inputs).logits"
      ],
      "metadata": {
        "id": "9IUUpb6WIqHY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}